{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to minimize the function\n",
    "\\begin{align}\n",
    "    \\mathcal{G}(\\mu) = \\mathcal{E}_F(\\mu) + \\int\\log(\\mu(x))d\\mu(x).\n",
    "\\end{align}\n",
    "If we define\n",
    "$$\n",
    "F(x) = 0.5|x|^2\n",
    "$$\n",
    "and\n",
    "$$\n",
    "H(\\mu) = \\int\\log(\\mu(x))d\\mu(x)\n",
    "$$\n",
    "then note that we are going to minimize $\\mathcal{G}$ which is merely an integral over a quadratic function according to the measure $\\mu$, regularized by negative entropy. We want to find the measure that minimizes this value. In our case specifically, this minimization amounts to minimizing the KL divergence, i.e., $\\mathcal{G}(\\mu)$ is the KL divergence between $\\mu$ and $\\mu_*$ where $\\mu_*$ is standard Gaussian distribution. The idea is that we want to iteratively update the measure $\\mu_n$ according to the proximal-gradient update scheme\n",
    "\\begin{align}\n",
    "    \\nu_{n+1} &= (I - \\gamma\\nabla F)_{\\#}\\mu_n\\\\\n",
    "    \\mu_{n+1} &\\in \\mathrm{JKO}_{\\gamma H}(\\nu_{n+1}).\n",
    "\\end{align}\n",
    "The idea is that as $n\\to\\infty$, we have that $\\mu_n \\to \\mu_*$. The authors give convergence guarantees and convergence rates which we're not going to look at here. But, in our specific example, we know that this update scheme is equivalent to\n",
    "\\begin{align}\n",
    "    m_{k+1} &= m + (I-\\gamma\\Sigma^{-1})(\\mu_k - \\mu)\\\\\n",
    "    \\Sigma_{k+1}(I - \\gamma\\Sigma_{k+1}^{-1})^2 &= \\Sigma_k (I-\\gamma\\Sigma^{-1}),\n",
    "\\end{align}\n",
    "where $m$ represents the mean and $\\Sigma$ represents the variance (covariance matrix specifically of the iterative distribution). We may use this update structure directly on the mean and variance since every iterative distributional update $\\mu_n$ is Gaussian necessarily and because we have a closed form for the $\\mathrm{JKO}$ operator. So, in our example, we resort to 1D optimal transport through these iterative updates (which we may perform specifically on just the mean and variance through these updates), we have $m_* = 0$ and $\\sigma_* = 1$, and initialize with $m_0 = 10$ and $\\sigma_0 = 100$. At every step, we also have a closed form expression for the Wasserstein-2 distance $W^2 (\\mu_n, \\mu_*) = m_{n}^2 + (1 - \\sqrt{\\sigma_n})^2$. We now code all of this up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
